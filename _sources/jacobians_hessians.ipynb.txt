{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98c5346d-c11a-4be1-8a20-447d7390fdd9",
   "metadata": {},
   "source": [
    "# Jacobians, hessians, and more: composing functorch transforms\n",
    "\n",
    "Computing jacobians or hessians are useful in a number of non-traditional\n",
    "deep learning models. It is difficult (or annoying) to compute these quantities\n",
    "efficiently using a standard autodiff system like PyTorch Autograd; functorch\n",
    "provides ways of computing various higher-order autodiff quantities efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1aabaa9-6e86-4717-b645-b979a6b980a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "_ = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f033af1c-7d5d-4acf-a3b0-aa53c38f07c8",
   "metadata": {},
   "source": [
    "## Setup: Comparing functorch vs the naive approach\n",
    "\n",
    "Let's start with a function that we'd like to compute the jacobian of.\n",
    "This is a simple linear function with non-linear activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e056fcb-8cc3-4ea4-a0dd-0cb93beb07cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(weight, bias, x):\n",
    "    return F.linear(x, weight, bias).tanh()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987040aa-65a9-40eb-ab17-fa8a5078a9f8",
   "metadata": {},
   "source": [
    "Here's some dummy data: a weight, a bias, and a feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41c34504-5873-4861-b513-25bfc2e431b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 16\n",
    "weight = torch.randn(D, D)\n",
    "bias = torch.randn(D)\n",
    "x = torch.randn(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceff130-0765-47e9-aa59-de66b23651e0",
   "metadata": {},
   "source": [
    "Let's think of `predict` as a function that maps the input `x` from $R^D -> R^D$.\n",
    "PyTorch Autograd computes vector-Jacobian products. In order to compute the full\n",
    "Jacobian of this $R^D -> R^D$ function, we would have to compute it row-by-row\n",
    "by using a different unit vector each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b13ce38-1f4c-4b4e-b28c-94de44bf6cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xp = x.clone().requires_grad_()\n",
    "unit_vectors = torch.eye(D)\n",
    "\n",
    "def compute_jac(xp):\n",
    "    jacobian_rows = [torch.autograd.grad(predict(weight, bias, xp), xp, vec)[0]\n",
    "                     for vec in unit_vectors]\n",
    "    return torch.stack(jacobian_rows)\n",
    "\n",
    "jacobian = compute_jac(xp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fac9ba-a185-4478-bf29-1011c4e75ac0",
   "metadata": {},
   "source": [
    "Instead of computing the jacobian row-by-row, we can use `vmap` to get rid\n",
    "of the for-loop and vectorize the computation. We can't directly apply vmap\n",
    "to PyTorch Autograd; instead, functorch provides a `vjp` transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23e7861d-62a6-4b24-aac5-cca018002867",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functorch import vmap, vjp\n",
    "_, vjp_fn = vjp(partial(predict, weight, bias), x)\n",
    "ft_jacobian, = vmap(vjp_fn)(unit_vectors)\n",
    "assert torch.allclose(ft_jacobian, jacobian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c5396e-83cd-4c10-9cdc-8989605ff178",
   "metadata": {},
   "source": [
    "In another tutorial a composition of reverse-mode AD and vmap gave us\n",
    "per-sample-gradients. In this tutorial, composing reverse-mode AD and vmap\n",
    "gives us Jacobian computation! Various compositions of vmap and autodiff\n",
    "transforms can give us different interesting quantities.\n",
    "\n",
    "functorch provides `jacrev` as a convenience function that performs\n",
    "the vmap-vjp composition to compute jacobians. `jacrev` accepts an argnums\n",
    "argument that says which argument we would like to compute Jacobians with\n",
    "respect to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21d24a8c-69af-4c0e-b222-901c0f72182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functorch import jacrev\n",
    "ft_jacobian = jacrev(predict, argnums=2)(weight, bias, x)\n",
    "assert torch.allclose(ft_jacobian, jacobian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d55f7df-907e-4236-9104-c3d1b86ce935",
   "metadata": {},
   "source": [
    "Let's compare the performance of the two ways to compute jacobian.\n",
    "The functorch version is much faster (and becomes even faster the more outputs\n",
    "there are). In general, we expect that vectorization via `vmap` can help\n",
    "eliminate overhead and give better utilization of your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f502dc0-2faf-42b3-a9f3-d77737e94abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7f4462a8b3a0>\n",
      "compute_jac(xp)\n",
      "  1.08 ms\n",
      "  1 measurement, 500 runs , 1 thread\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7f4461e3ee20>\n",
      "jacrev(predict, argnums=2)(weight, bias, x)\n",
      "  361.07 us\n",
      "  1 measurement, 500 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.benchmark import Timer\n",
    "without_vmap = Timer(stmt=\"compute_jac(xp)\", globals=globals())\n",
    "with_vmap = Timer(stmt=\"jacrev(predict, argnums=2)(weight, bias, x)\", globals=globals())\n",
    "print(without_vmap.timeit(500))\n",
    "print(with_vmap.timeit(500))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fb7863-20f2-416b-9273-2c8ab2f7980c",
   "metadata": {},
   "source": [
    "Furthemore, it's pretty easy to flip the problem around and say we want to compute\n",
    "Jacobians of the parameters to our model (weight, bias) instead of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14d12ec7-c40f-42b9-b549-d79fac60d541",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_jac_weight, ft_jac_bias = jacrev(predict, argnums=(0, 1))(weight, bias, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a27d8b9-448b-4b73-9e11-f443fde24f6f",
   "metadata": {},
   "source": [
    "## reverse-mode Jacobian (jacrev) vs forward-mode Jacobian (jacfwd)\n",
    "\n",
    "We offer two APIs to compute jacobians: jacrev and jacfwd:\n",
    "- jacrev uses reverse-mode AD. As you saw above it is a composition of our\n",
    "vjp and vmap transforms.\n",
    "- jacfwd uses forward-mode AD. It is implemented as a composition of our\n",
    "jvp and vmap transforms.\n",
    "jacfwd and jacrev can be subsituted for each other and have different\n",
    "performance characteristics.\n",
    "\n",
    "As a general rule of thumb, if you're computing the jacobian of an $R^N -> R^M$\n",
    "function, if there are many more outputs than inputs (i.e. M > N) then jacfwd is\n",
    "preferred, otherwise use jacrev. There are exceptions to this rule, but a\n",
    "non-rigorous argument for this follows:\n",
    "\n",
    "In reverse-mode AD, we are computing the jacobian row-by-row, while in\n",
    "forward-mode AD (which computes Jacobian-vector products), we are computing\n",
    "it column-by-column. The Jacobian matrix has M rows and N columns, so if it is\n",
    "taller or wider one way we may prefer the method that deals with fewer rows or\n",
    "columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "201c6470-0e35-4e5b-a3c0-cb602da8ca5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functorch import jacrev, jacfwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bd6c60-5cb7-4628-a63c-2ebf9d9e1cff",
   "metadata": {},
   "source": [
    "Benchmark with more inputs than outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08a83c44-9e3f-4734-9d25-9844a5691791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jacfwd time: <torch.utils.benchmark.utils.common.Measurement object at 0x7f44629bc760>\n",
      "jacfwd(predict, argnums=2)(weight, bias, x)\n",
      "  603.91 us\n",
      "  1 measurement, 500 runs , 1 thread\n",
      "jacrev time: <torch.utils.benchmark.utils.common.Measurement object at 0x7f4461e1b8b0>\n",
      "jacrev(predict, argnums=2)(weight, bias, x)\n",
      "  5.25 ms\n",
      "  1 measurement, 500 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "Din = 32\n",
    "Dout = 2048\n",
    "weight = torch.randn(Dout, Din)\n",
    "bias = torch.randn(Dout)\n",
    "x = torch.randn(Din)\n",
    "\n",
    "using_fwd = Timer(stmt=\"jacfwd(predict, argnums=2)(weight, bias, x)\", globals=globals())\n",
    "using_bwd = Timer(stmt=\"jacrev(predict, argnums=2)(weight, bias, x)\", globals=globals())\n",
    "print(f'jacfwd time: {using_fwd.timeit(500)}')\n",
    "print(f'jacrev time: {using_bwd.timeit(500)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66984d8c-135c-49aa-b177-655836f87e3c",
   "metadata": {},
   "source": [
    "Benchmark with more outputs than inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18029e78-c722-4163-af56-79b820074bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jacfwd time: <torch.utils.benchmark.utils.common.Measurement object at 0x7f4461e19a60>\n",
      "jacfwd(predict, argnums=2)(weight, bias, x)\n",
      "  5.33 ms\n",
      "  1 measurement, 500 runs , 1 thread\n",
      "jacrev time: <torch.utils.benchmark.utils.common.Measurement object at 0x7f4461e30ee0>\n",
      "jacrev(predict, argnums=2)(weight, bias, x)\n",
      "  424.29 us\n",
      "  1 measurement, 500 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "Din = 2048\n",
    "Dout = 32\n",
    "weight = torch.randn(Dout, Din)\n",
    "bias = torch.randn(Dout)\n",
    "x = torch.randn(Din)\n",
    "\n",
    "using_fwd = Timer(stmt=\"jacfwd(predict, argnums=2)(weight, bias, x)\", globals=globals())\n",
    "using_bwd = Timer(stmt=\"jacrev(predict, argnums=2)(weight, bias, x)\", globals=globals())\n",
    "print(f'jacfwd time: {using_fwd.timeit(500)}')\n",
    "print(f'jacrev time: {using_bwd.timeit(500)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7612bcdd-f93d-4c7a-89f6-0a734a7aca5b",
   "metadata": {},
   "source": [
    "## Hessian computation with functorch.hessian\n",
    "\n",
    "We offer a convenience API to compute hessians: functorch.hessian.\n",
    "Hessians are the jacobian of the jacobian, which suggests that one can just\n",
    "compose functorch's jacobian transforms to compute one.\n",
    "Indeed, under the hood, ``hessian(f)`` is simply ``jacfwd(jacrev(f))``\n",
    "\n",
    "Depending on your model, you may also want to use `jacfwd(jacfwd(f))` or\n",
    "`jacrev(jacrev(f))` instead to compute hessians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "845118a5-b923-48f2-adbc-9509efd9143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functorch import hessian\n",
    "# # TODO(rzou): make sure PyTorch has tanh_backward implemented for jvp!!\n",
    "# hess0 = hessian(predict, argnums=2)(weight, bias, x)\n",
    "# hess1 = jacfwd(jacfwd(predict, argnums=2), argnums=2)(weight, bias, x)\n",
    "hess2 = jacrev(jacrev(predict, argnums=2), argnums=2)(weight, bias, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dd11da-19c0-4f82-8710-fc92c81967ea",
   "metadata": {},
   "source": [
    "## Batch Jacobian (and Batch Hessian)\n",
    "\n",
    "In the above examples we've been operating with a single feature vector.\n",
    "In some cases you might want to take the Jacobian of a batch of outputs\n",
    "with respect to a batch of inputs. That is, given a batch of inputs of shape `(B, N)` and a function\n",
    "that goes from `R^N -> R^M`, we would like a Jacobian of shape `(B, M, N)`.\n",
    "The easiest way to do this is to use vmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af55d512-b8cf-4304-8d2c-2df88a0352ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "Din = 31\n",
    "Dout = 33\n",
    "weight = torch.randn(Dout, Din)\n",
    "bias = torch.randn(Dout)\n",
    "x = torch.randn(batch_size, Din)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2a159d6-6724-4086-bc5c-490d1e7515f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_batch_jacobian = vmap(jacrev(predict, argnums=2), in_dims=(None, None, 0))\n",
    "batch_jacobian0 = compute_batch_jacobian(weight, bias, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fae848-d3d7-4148-93d4-ed609372e14f",
   "metadata": {},
   "source": [
    "If you have a function that goes from `(B, N) -> (B, M)` instead and are certain that each input produces an independent\n",
    "output, then it's also sometimes possible to do this without using `vmap` by summing the outputs and then computing the Jacobian of that function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13de15ed-d2f7-4733-816e-eb265aa83429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_output_summed(weight, bias, x):\n",
    "    return predict(weight, bias, x).sum(0)\n",
    "\n",
    "batch_jacobian1 = jacrev(predict_with_output_summed, argnums=2)(weight, bias, x).movedim(1, 0)\n",
    "assert torch.allclose(batch_jacobian0, batch_jacobian1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15adba4-e8ab-482f-bfa6-bdd6055eb7fb",
   "metadata": {},
   "source": [
    "If you instead have a function that goes from $R^N -> R^M$ but inputs that are\n",
    "batched, you compose vmap with jacrev to compute batched jacobians:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d257bbb-d208-40be-9982-6b88a0eb9f3a",
   "metadata": {},
   "source": [
    "Finally, batch hessians can be computed similarly. It's easiest to think about\n",
    "them by using vmap to batch over hessian computation, but in some cases the sum\n",
    "trick also works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10353024-d0db-4c63-8865-c21973dcfc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_batch_hessian = vmap(hessian(predict, argnums=2), in_dims=(None, None, 0))\n",
    "# TODO(rzou): PyTorch forward-mode AD does not support tanh_backward\n",
    "# batch_hess = compute_batch_hessian(weight, bias, x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
